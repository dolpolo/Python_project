{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIDEN vs TRUMP: are tweeters a good proxy for USA elections?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import plotly.express as px \n",
    "import plotly.io as pio\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Libraries for Sentiment Analysis \n",
    "import re \n",
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.corpus import wordnet \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from textblob import TextBlob \n",
    "from wordcloud import WordCloud \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: C:\\Users\\Davide\\Desktop\\Alma Mater\\SECOND YEAR\\PYTHON\\Python_project\n"
     ]
    }
   ],
   "source": [
    "# Define the base path\n",
    "base_path = Path(\"C:/Users/Davide/Desktop/Alma Mater/SECOND YEAR/PYTHON/Python_project\")\n",
    "# Change the working directory\n",
    "os.chdir(base_path)\n",
    "\n",
    "# Define the full path to the CSV file for Trump and Biden\n",
    "merged_data = base_path / \"data\" / \"data.csv\"\n",
    "\n",
    "# Print the current working directory\n",
    "print(\"Current Working Directory:\", Path.cwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the DataFrame:\n",
      "   index           created_at      tweet_id  \\\n",
      "0      0  2020-10-15 00:00:01  1.316529e+18   \n",
      "1      2  2020-10-15 00:00:02  1.316529e+18   \n",
      "2      4  2020-10-15 00:00:08  1.316529e+18   \n",
      "3      5  2020-10-15 00:00:17  1.316529e+18   \n",
      "4      7  2020-10-15 00:00:18  1.316529e+18   \n",
      "\n",
      "                                               tweet  likes  retweet_count  \\\n",
      "0  #Elecciones2020 | En #Florida: #JoeBiden dice ...    0.0            0.0   \n",
      "1  #Trump: As a student I used to hear for years,...    2.0            1.0   \n",
      "2  You get a tie! And you get a tie! #Trump ‚Äòs ra...    4.0            3.0   \n",
      "3  @CLady62 Her 15 minutes were over long time ag...    2.0            0.0   \n",
      "4  @DeeviousDenise @realDonaldTrump @nypost There...    0.0            0.0   \n",
      "\n",
      "                source       user_id  \\\n",
      "0            TweetDeck  3.606665e+08   \n",
      "1      Twitter Web App  8.436472e+06   \n",
      "2   Twitter for iPhone  4.741380e+07   \n",
      "3  Twitter for Android  1.138416e+09   \n",
      "4   Twitter for iPhone  9.007611e+17   \n",
      "\n",
      "                                    user_name user_screen_name  ...  \\\n",
      "0                          El Sol Latino News  elsollatinonews  ...   \n",
      "1                                      snarke           snarke  ...   \n",
      "2                       Rana Abtar - ÿ±ŸÜÿß ÿ£ÿ®ÿ™ÿ±        Ranaabtar  ...   \n",
      "3                                Farris Flagg      FarrisFlagg  ...   \n",
      "4  Stacey Gulledge üá∫üá∏ Patriot ‚ô•Ô∏è KAG üôè üëÆ‚Äç‚ôÄÔ∏è‚ô•Ô∏è      sm_gulledge  ...   \n",
      "\n",
      "                  user_location        lat        long        city  country  \\\n",
      "0  Philadelphia, PA / Miami, FL  25.774270  -80.193660         NaN       US   \n",
      "1                      Portland  45.520247 -122.674195    Portland       US   \n",
      "2                 Washington DC  38.894992  -77.036558  Washington       US   \n",
      "3             Perris,California  33.782519 -117.228648         NaN       US   \n",
      "4                     Ohio, USA  40.225357  -82.688140         NaN       US   \n",
      "\n",
      "       continent                 state state_code  \\\n",
      "0  North America               Florida         FL   \n",
      "1  North America                Oregon         OR   \n",
      "2  North America  District of Columbia         DC   \n",
      "3  North America            California         CA   \n",
      "4  North America                  Ohio         OH   \n",
      "\n",
      "                    collected_at candidate  \n",
      "0            2020-10-21 00:00:00     trump  \n",
      "1  2020-10-21 00:00:00.746433060     trump  \n",
      "2  2020-10-21 00:00:01.492866121     trump  \n",
      "3  2020-10-21 00:00:01.866082651     trump  \n",
      "4  2020-10-21 00:00:02.612515712     trump  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    data = pd.read_csv(merged_data, encoding=\"utf-8\", engine='python', on_bad_lines='skip')\n",
    "    print(\"First 5 rows of the DataFrame:\")\n",
    "    print(data.head())\n",
    "except Exception as e:\n",
    "    print(\"Error loading the file:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Analysis Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "turn the tweets into lowercase latters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of 5 tweets after converting to lowercase:\n",
      "316761    @joebiden #democrats #biden https://t.co/ufihk...\n",
      "355588    \"ga and pa\" i'm not going to miss all the lies...\n",
      "266439    this quote is for my family of 4. i can't affo...\n",
      "106352        #foxnews #maga #trump https://t.co/0q5ylkzjj0\n",
      "292481    in other news, the biden supporter left strand...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data['tweet'] = data['tweet'].str.lower()  \n",
    "print(\"Sample of 5 tweets after converting to lowercase:\")\n",
    "print(data['tweet'].sample(5))  # Check randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets not in lowercase: 135\n",
      "Sample of DataFrame after dropping uppercase tweets:\n",
      "         index           created_at      tweet_id  \\\n",
      "276851  175590  2020-10-25 17:04:37  1.320411e+18   \n",
      "255047  113656  2020-10-23 01:34:00  1.319452e+18   \n",
      "309817  274377  2020-11-01 13:25:15  1.322892e+18   \n",
      "109880  350142  2020-10-30 16:00:07  1.322207e+18   \n",
      "280281  185663  2020-10-26 11:55:22  1.320696e+18   \n",
      "\n",
      "                                                    tweet  likes  \\\n",
      "276851  #biden answers to his donors. @realdonaldtrump...    0.0   \n",
      "255047  @joebiden #biden we already know, you cannot d...    5.0   \n",
      "309817  @morethanmysle @hunterjcullen üá∫üá∏üíôüá∫üá∏ i voted #j...    0.0   \n",
      "109880  9:15am = avg 1st #trump tweet this week (edt)....   10.0   \n",
      "280281  i wonder who #joebiden is running against today??    7.0   \n",
      "\n",
      "        retweet_count               source       user_id  \\\n",
      "276851            0.0  Twitter for Android  8.258398e+17   \n",
      "255047            0.0   Twitter for iPhone  1.698183e+09   \n",
      "309817            0.0   Twitter for iPhone  9.791153e+17   \n",
      "109880            3.0    TrumpTweetWeather  9.344748e+17   \n",
      "280281            1.0   Twitter for iPhone  1.859305e+09   \n",
      "\n",
      "                            user_name user_screen_name  ...  \\\n",
      "276851                     JTGS Gamer         rlburson  ...   \n",
      "255047  ‚ö†Ô∏è‚úùÔ∏èüá∫üá∏Shawn Littlefield‚úùÔ∏èüá∫üá∏‚ö†Ô∏è         slittlef  ...   \n",
      "309817              Janet Grant Burns      jgrantburns  ...   \n",
      "109880          Trump Weather Service  realtrumpweathr  ...   \n",
      "280281                  3rdeyewinking    3rdeyewinking  ...   \n",
      "\n",
      "            user_location        lat        long         city  country  \\\n",
      "276851      Missouri, USA  38.760481  -92.561787          NaN       US   \n",
      "255047        Lubbock, TX  33.563521 -101.879336      Lubbock       US   \n",
      "309817     Huntsville, AL  34.729847  -86.585901   Huntsville       US   \n",
      "109880      United States  39.783730 -100.445882          NaN       US   \n",
      "280281  San Antonio Texas  29.424600  -98.495141  San Antonio       US   \n",
      "\n",
      "            continent     state state_code                collected_at  \\\n",
      "276851  North America  Missouri         MO  2020-10-27 18:18:54.704090   \n",
      "255047  North America     Texas         TX  2020-10-27 18:27:15.841852   \n",
      "309817  North America   Alabama         AL  2020-11-02 16:56:24.752443   \n",
      "109880  North America       NaN        NaN  2020-11-01 10:57:27.054703   \n",
      "280281  North America     Texas         TX  2020-10-27 18:04:39.941158   \n",
      "\n",
      "       candidate  \n",
      "276851     biden  \n",
      "255047     biden  \n",
      "309817     biden  \n",
      "109880     trump  \n",
      "280281     biden  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check if each tweet is composed only of lowercase characters\n",
    "all_lowercase = data['tweet'].apply(lambda x: x.islower())\n",
    "\n",
    "# Count the number of tweets that are not in lowercase\n",
    "non_lowercase_count = all_lowercase.value_counts().get(False, 0)  # Count False values\n",
    "print(\"Number of tweets not in lowercase:\", non_lowercase_count)\n",
    "\n",
    "# Drop rows where the tweet is still in uppercase\n",
    "data = data[~data['tweet'].str.isupper()]\n",
    "\n",
    "# Check the cleaned DataFrame\n",
    "print(\"Sample of DataFrame after dropping uppercase tweets:\")\n",
    "print(data.sample(5))  # Display a sample of the cleaned DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of tokenized tweets:\n",
      "176502    [[usa] jueces de georgia y michigan, desestima...\n",
      "240313    [finally the #secretempires\\nof #biden crimes ...\n",
      "33510     [@thehill he just reveals himself to be the \"d...\n",
      "372986    [#biden hablar√° a las 8pm., https://t.co/hnivz...\n",
      "373409    [#biden #election2020 is projected the üèÜ\\nhttp...\n",
      "Name: tokenized_tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data['tokenized_tweet'] = data['tweet'].apply(sent_tokenize)\n",
    "print(\"Sample of tokenized tweets:\")\n",
    "print(data['tokenized_tweet'].sample(5))  # Verifica le prime righe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of tokenized tweets after punctuation removal:\n",
      "170617    [anncoulter  there is no evidence that any siz...\n",
      "155500    [even now its close, but its looking trump adv...\n",
      "198949    [of course he was  not like he still has a job...\n",
      "64132     [heres the video of most of trumps remarks aft...\n",
      "390845    [heres what to expect with the new us presiden...\n",
      "Name: tokenized_tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Funzione per rimuovere la punteggiatura\n",
    "def remove_punctuation(tokens):\n",
    "    return [re.sub(r'[^\\w\\s]', '', word) for word in tokens]\n",
    "\n",
    "# Rimuovi la punteggiatura dalla colonna 'tokenized_tweet'\n",
    "data['tokenized_tweet'] = data['tokenized_tweet'].apply(remove_punctuation)\n",
    "print(\"Sample of tokenized tweets after punctuation removal:\")\n",
    "print(data['tokenized_tweet'].sample(5))  # Verifica le prime righe aggiornate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of word tokenized tweets:\n",
      "217402    [[newsweek, the, difference, biden, is, not, a...\n",
      "175167    [[realdonaldtrump], [realdonaldtrump, psycho, ...\n",
      "73079     [[trump, says, biden, will, burn, down, the, c...\n",
      "199588    [[panawahpskek, as, notorious, biggie, said, i...\n",
      "327308    [[mike_pence, realdonaldtrump, 230000, america...\n",
      "Name: tokenized_tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "tokenized_words = data['tokenized_tweet'].apply(lambda x: [word_tokenize(sentence) for sentence in x])\n",
    "print(\"Sample of word tokenized tweets:\")\n",
    "print(tokenized_words.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flate the column tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 words from the flattened list:\n",
      "[['elecciones2020', 'en', 'florida', 'joebiden', 'dice', 'que', 'donaldtrump', 'solo', 'se', 'preocupa', 'por', '√©l', 'mismo'], ['el', 'dem√≥crata', 'fue', 'anfitri√≥n', 'de', 'encuentros', 'de', 'electores', 'en', 'pembrokepines', 'y', 'miramar'], ['clic', 'aqu√≠', 'httpstcoqhiwpiuxst', '_', 'elsollatino', 'yobrilloconelsol', 'httpstco6flcbwf1mi'], ['trump', 'as', 'a', 'student', 'i', 'used', 'to', 'hear', 'for', 'years', 'for', 'ten', 'years', 'i', 'heard', 'china'], ['in', '2019'], ['and', 'we', 'have', '15', 'and', 'they', 'dont', 'know', 'how', 'many', 'we', 'have', 'and', 'i', 'asked', 'them', 'how', 'many', 'do', 'we', 'have', 'and', 'they', 'said', 'sir', 'we', 'dont', 'know'], ['but', 'we', 'have', 'millions'], ['like', '300', 'million'], ['um'], ['what']]\n",
      "First 10 words from the one list:\n",
      "['elecciones2020', 'en', 'florida', 'joebiden', 'dice', 'que', 'donaldtrump', 'solo', 'se', 'preocupa', 'por', '√©l', 'mismo', 'el', 'dem√≥crata', 'fue', 'anfitri√≥n', 'de', 'encuentros', 'de', 'electores', 'en', 'pembrokepines', 'y', 'miramar', 'clic', 'aqu√≠', 'httpstcoqhiwpiuxst', '_', 'elsollatino', 'yobrilloconelsol', 'httpstco6flcbwf1mi', 'trump', 'as', 'a', 'student', 'i', 'used', 'to', 'hear', 'for', 'years', 'for', 'ten', 'years', 'i', 'heard', 'china', 'in', '2019', 'and', 'we', 'have', '15', 'and', 'they', 'dont', 'know', 'how', 'many', 'we', 'have', 'and', 'i', 'asked', 'them', 'how', 'many', 'do', 'we', 'have', 'and', 'they', 'said', 'sir', 'we', 'dont', 'know', 'but', 'we', 'have', 'millions', 'like', '300', 'million', 'um', 'what', 'you', 'get', 'a', 'tie', 'and', 'you', 'get', 'a', 'tie', 'trump', 's', 'rally', 'iowa']\n"
     ]
    }
   ],
   "source": [
    "# Appiattiamo la lista di liste in una lista di parole\n",
    "all_words_flat = [word for sublist in tokenized_words for word in sublist]\n",
    "all_word_list = [word for sublist in all_words_flat for word in sublist]\n",
    "print(\"First 10 words from the flattened list:\")\n",
    "print(all_words_flat[:10])  # Stampa le prime 10 parole\n",
    "print(\"First 10 words from the one list:\")\n",
    "print(all_word_list[:100])  # Stampa le prime 10 parole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"you're\", 'shan', 'if', 'here', \"hasn't\", 'now', 'i', 'whom', \"you'll\", 'have', 'had', \"doesn't\", 'being', 'with', 'those', 'by', 'at', 'doesn', \"couldn't\", \"wasn't\", 'you', 'out', 'all', 'our', 'into', 'above', 'its', \"didn't\", 'hers', 'which', \"shouldn't\", 'in', 'am', 'm', 'to', 'when', 'hadn', 'because', \"it's\", 're', 'needn', \"you'd\", 'other', 'ma', 'doing', 'are', 'having', 'off', \"hadn't\", 'me', 'does', 'him', 'she', 'then', 'he', 'so', \"shan't\", \"that'll\", 'there', 'than', 'about', 'yours', 'ain', \"mustn't\", 'won', 'only', \"should've\", 'yourselves', 'who', 'yourself', 'just', 'has', 'through', 'until', 'few', 'haven', \"won't\", 'where', 'nor', 'were', 'isn', 'will', 'itself', 'my', 'what', 'myself', 'own', 'same', 'that', \"aren't\", 'most', 'did', 'themselves', 'further', 'but', 't', 'they', 'between', 'hasn', \"she's\", 'as', 'once', 'your', 'should', 'd', 'a', 'don', 'll', 'while', 'weren', 'this', 'any', 'it', 'her', 'an', \"isn't\", \"haven't\", 've', 'both', 'his', 'down', 'and', 'why', 'aren', 'how', 'mustn', 'can', 'some', 'more', 'during', \"mightn't\", 'for', \"wouldn't\", 'or', 'didn', 'we', 'their', 'too', 'up', 's', 'y', 'is', 'wouldn', 'from', 'no', \"weren't\", 'couldn', \"you've\", 'before', 'ourselves', 'after', 'each', 'mightn', 'ours', 'was', 'shouldn', 'the', 'himself', 'do', 'on', 'be', 'not', 'theirs', 'below', \"needn't\", 'of', 'under', 'again', 'been', 'very', 'wasn', 'o', \"don't\", 'against', 'such', 'over', 'these', 'them', 'herself'}\n"
     ]
    }
   ],
   "source": [
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Filtered Sentence: ['elecciones2020', 'en', 'florida', 'joebiden', 'dice', 'que', 'donaldtrump', 'solo', 'se', 'preocupa', 'por', '√©l', 'mismo', 'el', 'dem√≥crata', 'fue', 'anfitri√≥n', 'de', 'encuentros', 'de', 'electores', 'en', 'pembrokepines', 'miramar', 'clic', 'aqu√≠', 'httpstcoqhiwpiuxst', '_', 'elsollatino', 'yobrilloconelsol', 'httpstco6flcbwf1mi', 'trump', 'student', 'used', 'hear', 'years', 'ten', 'years', 'heard', 'china', '2019', '15', 'dont', 'know', 'many', 'asked', 'many', 'said', 'sir', 'dont', 'know', 'millions', 'like', '300', 'million', 'um', 'get', 'tie', 'get', 'tie', 'trump', 'rally', 'iowa', 'httpstcojjaluumh5d', 'clady62', '15', 'minutes', 'long', 'time', 'ago', 'omarosa', 'never', 'represented', 'black', 'community', 'thereidout', 'cried', 'trump', 'begging', 'job', 'deeviousdenise', 'realdonaldtrump', 'nypost', 'wont', 'many', 'unless', 'voting', 'god', 'prevails', 'bo', 'corrupt', 'president', 'ever', 'dark', 'light', 'lies', 'coming', 'wouldnt', 'last', 'forever']\n"
     ]
    }
   ],
   "source": [
    "# Rimuovi le stopword \n",
    "filtered_word = []\n",
    "for w in all_word_list:\n",
    "    if w not in stop_words:\n",
    "        filtered_word.append(w)\n",
    "\n",
    "# Salva solo le prime 100 parole filtrate\n",
    "filtered_word = filtered_word[:100]\n",
    "\n",
    "print(\"\\n\\nFiltered Sentence:\", filtered_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEMMATIZATION????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FREQUENCY ANALYSIS ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall only take into account US citizens‚Äô opinions here, as they are a crucial deciding factor in who becomes the US president."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text): \n",
    "\t# Remove URLs \n",
    "\ttext = re.sub(r'https?://\\S+|www\\.\\S+', '', str(text)) \n",
    "\n",
    "\t# Convert text to lowercase \n",
    "\ttext = text.lower() \n",
    "\n",
    "\t# Replace anything other than alphabets a-z with a space \n",
    "\ttext = re.sub('[^a-z]', ' ', text) \n",
    "\n",
    "\t# Split the text into single words \n",
    "\ttext = text.split() \n",
    "\n",
    "\t# Initialize WordNetLemmatizer \n",
    "\tlm = WordNetLemmatizer() \n",
    "\n",
    "\t# Lemmatize words and remove stopwords \n",
    "\ttext = [lm.lemmatize(word) for word in text if word not in set( \n",
    "\t\tstopwords.words('english'))] \n",
    "\n",
    "\t# Join the words back into a sentence \n",
    "\ttext = ' '.join(word for word in text) \n",
    "\n",
    "\treturn text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get polarity, subjectivity, and Analysis Now, let‚Äôs create a function to get polarity, subjectivity, and Analysis function to fetch sentiments from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getpolarity(text): \n",
    "    return TextBlob(text).sentiment.polarity \n",
    "\n",
    "def getsubjectivity(text): \n",
    "    return TextBlob(text).sentiment.subjectivity \n",
    "\n",
    "def getAnalysis(score): \n",
    "    if score < 0: \n",
    "        return 'negative'\n",
    "    elif score == 0: \n",
    "        return 'neutral'\n",
    "    else: \n",
    "        return 'positive'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADD THE FREQUENCY ANALYSIS FOR BIDEN AND TRUMP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
